# LightRAG é¡¹ç›®ä»£ç å…¨é¢åˆ†æ

æœ¬æ–‡æ¡£å¯¹ LightRAG (Python) é¡¹ç›®è¿›è¡Œå…¨é¢æ·±å…¥çš„ä»£ç åˆ†æï¼Œè¯¦ç»†è§£é‡Šå„æ¨¡å—çš„åŠŸèƒ½ã€æ•°æ®æµå‘ã€ä»¥åŠå­˜å‚¨æœºåˆ¶ã€‚

---

## 1. é¡¹ç›®æ•´ä½“æ¶æ„

### 1.1 ç›®å½•ç»“æ„

```
lightrag/
â”œâ”€â”€ __init__.py           # åŒ…å…¥å£
â”œâ”€â”€ lightrag.py          # ä¸»ç±» LightRAG (4059 è¡Œ)
â”œâ”€â”€ operate.py           # æ ¸å¿ƒæ“ä½œé€»è¾‘ (5003 è¡Œ)
â”œâ”€â”€ base.py              # æŠ½è±¡å­˜å‚¨æ¥å£ (908 è¡Œ)
â”œâ”€â”€ prompt.py            # LLM æç¤ºè¯æ¨¡æ¿
â”œâ”€â”€ constants.py         # å…¨å±€å¸¸é‡é…ç½®
â”œâ”€â”€ utils.py             # å·¥å…·å‡½æ•° (3353 è¡Œ)
â”œâ”€â”€ utils_graph.py       # å›¾ç›¸å…³å·¥å…·
â”œâ”€â”€ types.py             # ç±»å‹å®šä¹‰
â”œâ”€â”€ exceptions.py        # è‡ªå®šä¹‰å¼‚å¸¸
â”œâ”€â”€ namespace.py         # å‘½åç©ºé—´ç®¡ç†
â”œâ”€â”€ rerank.py            # é‡æ’åºåŠŸèƒ½
â”œâ”€â”€ kg/                  # çŸ¥è¯†å›¾è°±å­˜å‚¨å®ç°
â”‚   â”œâ”€â”€ __init__.py      # å­˜å‚¨æ³¨å†Œè¡¨
â”‚   â”œâ”€â”€ shared_storage.py # å…±äº«å­˜å‚¨ç®¡ç† (é”ã€å‘½åç©ºé—´)
â”‚   â”œâ”€â”€ json_kv_impl.py  # JSON KV å­˜å‚¨
â”‚   â”œâ”€â”€ nano_vector_db_impl.py  # NanoVectorDB å‘é‡å­˜å‚¨
â”‚   â”œâ”€â”€ networkx_impl.py # NetworkX å›¾å­˜å‚¨
â”‚   â”œâ”€â”€ neo4j_impl.py    # Neo4j å›¾å­˜å‚¨
â”‚   â”œâ”€â”€ milvus_impl.py   # Milvus å‘é‡å­˜å‚¨
â”‚   â”œâ”€â”€ postgres_impl.py # PostgreSQL å­˜å‚¨
â”‚   â”œâ”€â”€ mongo_impl.py    # MongoDB å­˜å‚¨
â”‚   â”œâ”€â”€ redis_impl.py    # Redis å­˜å‚¨
â”‚   â”œâ”€â”€ qdrant_impl.py   # Qdrant å‘é‡å­˜å‚¨
â”‚   â””â”€â”€ ...
â”œâ”€â”€ llm/                 # LLM æ¨¡å‹æ¥å£
â”‚   â”œâ”€â”€ openai.py
â”‚   â”œâ”€â”€ anthropic.py
â”‚   â”œâ”€â”€ gemini.py
â”‚   â”œâ”€â”€ ollama.py
â”‚   â””â”€â”€ ...
â””â”€â”€ api/                 # REST API æœåŠ¡
    â””â”€â”€ lightrag_server.py
```

### 1.2 æ ¸å¿ƒæ¶æ„å›¾

```mermaid
flowchart TB
    subgraph ç”¨æˆ·å±‚
        U1[ğŸ“„ æ–‡æ¡£è¾“å…¥]
        U2[â“ æŸ¥è¯¢è¯·æ±‚]
    end

    subgraph LightRAG æ ¸å¿ƒ
        LR[LightRAG ä¸»ç±»]

        subgraph å¤„ç†å±‚
            OP1[ğŸ“ chunking_by_token_size<br/>æ–‡æœ¬åˆ†å—]
            OP2[ğŸ”¬ extract_entities<br/>å®ä½“æå–]
            OP3[ğŸ”— merge_nodes_and_edges<br/>å›¾è°±åˆå¹¶]
            OP4[ğŸ¯ kg_query<br/>çŸ¥è¯†æŸ¥è¯¢]
        end

        subgraph LLM å±‚
            LLM1[ğŸ¤– llm_model_func<br/>æ–‡æœ¬ç”Ÿæˆ]
            LLM2[ğŸ”¢ embedding_func<br/>å‘é‡åµŒå…¥]
        end

        subgraph å­˜å‚¨å±‚
            subgraph KV å­˜å‚¨
                KV1[(docs_kv<br/>æ–‡æ¡£çŠ¶æ€)]
                KV2[(chunks_kv<br/>æ–‡æœ¬å—)]
                KV3[(llm_cache<br/>LLM ç¼“å­˜)]
                KV4[(entity_chunks<br/>å®ä½“-å—æ˜ å°„)]
                KV5[(relation_chunks<br/>å…³ç³»-å—æ˜ å°„)]
            end

            subgraph å‘é‡å­˜å‚¨
                VDB1[(entities_vdb<br/>å®ä½“å‘é‡)]
                VDB2[(relations_vdb<br/>å…³ç³»å‘é‡)]
                VDB3[(chunks_vdb<br/>å—å‘é‡)]
            end

            subgraph å›¾å­˜å‚¨
                GS[(graph_storage<br/>çŸ¥è¯†å›¾è°±)]
            end
        end
    end

    U1 --> LR
    U2 --> LR
    LR --> OP1
    OP1 --> OP2
    OP2 --> LLM1
    OP2 --> OP3
    OP3 --> LLM2
    OP3 --> GS
    OP3 --> VDB1
    OP3 --> VDB2
    LR --> OP4
    OP4 --> VDB1
    OP4 --> VDB2
    OP4 --> VDB3
    OP4 --> GS
    OP4 --> LLM1
```

---

## 2. å­˜å‚¨ç±»å‹è¯¦è§£

LightRAG ä½¿ç”¨å››ç§ç±»å‹çš„å­˜å‚¨ç³»ç»Ÿï¼Œæ¯ç§éƒ½æœ‰å¤šä¸ªå®ç°é€‰é¡¹ï¼š

### 2.1 å­˜å‚¨ç±»å‹æ³¨å†Œè¡¨

```python
# æ¥è‡ª kg/__init__.py

STORAGE_IMPLEMENTATIONS = {
    "KV_STORAGE": {
        "implementations": [
            "JsonKVStorage",      # åŸºäº JSON æ–‡ä»¶
            "RedisKVStorage",     # Redis
            "PGKVStorage",        # PostgreSQL
            "MongoKVStorage",     # MongoDB
        ],
        "required_methods": ["get_by_id", "upsert"],
    },
    "GRAPH_STORAGE": {
        "implementations": [
            "NetworkXStorage",    # å†…å­˜ NetworkX å›¾
            "Neo4JStorage",       # Neo4j å›¾æ•°æ®åº“
            "PGGraphStorage",     # PostgreSQL å›¾
            "MongoGraphStorage",  # MongoDB å›¾
            "MemgraphStorage",    # Memgraph
        ],
        "required_methods": ["upsert_node", "upsert_edge"],
    },
    "VECTOR_STORAGE": {
        "implementations": [
            "NanoVectorDBStorage", # å†…å­˜ NanoVectorDB
            "MilvusVectorDBStorage", # Milvus
            "PGVectorStorage",    # PostgreSQL + pgvector
            "FaissVectorDBStorage", # FAISS
            "QdrantVectorDBStorage", # Qdrant
            "MongoVectorDBStorage", # MongoDB Atlas
        ],
        "required_methods": ["query", "upsert"],
    },
    "DOC_STATUS_STORAGE": {
        "implementations": [
            "JsonDocStatusStorage",
            "RedisDocStatusStorage",
            "PGDocStatusStorage",
            "MongoDocStatusStorage",
        ],
        "required_methods": ["get_docs_by_status"],
    },
}
```

### 2.2 å­˜å‚¨å®ä¾‹ä¸€è§ˆ

```mermaid
flowchart LR
    subgraph KV å­˜å‚¨å®ä¾‹
        A1[docs_kv] --> A1D[æ–‡æ¡£å…ƒæ•°æ®å’Œå¤„ç†çŠ¶æ€]
        A2[chunks_kv] --> A2D[æ–‡æœ¬å—å†…å®¹]
        A3[llm_cache] --> A3D[LLM å“åº”ç¼“å­˜]
        A4[entity_chunks_kv] --> A4D[å®ä½“åˆ°å—çš„æ˜ å°„]
        A5[relation_chunks_kv] --> A5D[å…³ç³»åˆ°å—çš„æ˜ å°„]
        A6[full_entities_kv] --> A6D[å®Œæ•´å®ä½“ä¿¡æ¯]
        A7[full_relations_kv] --> A7D[å®Œæ•´å…³ç³»ä¿¡æ¯]
    end

    subgraph å‘é‡å­˜å‚¨å®ä¾‹
        B1[entities_vdb] --> B1D[å®ä½“æè¿°çš„å‘é‡åµŒå…¥]
        B2[relations_vdb] --> B2D[å…³ç³»æè¿°çš„å‘é‡åµŒå…¥]
        B3[chunks_vdb] --> B3D[æ–‡æœ¬å—çš„å‘é‡åµŒå…¥]
    end

    subgraph å›¾å­˜å‚¨å®ä¾‹
        C1[graph_storage] --> C1D[çŸ¥è¯†å›¾è°±<br/>èŠ‚ç‚¹ + è¾¹]
    end
```

### 2.3 æ•°æ®æŒä¹…åŒ–ç»“æ„

ä½¿ç”¨ JSON å­˜å‚¨æ—¶ï¼Œæ•°æ®æ–‡ä»¶ç»“æ„å¦‚ä¸‹ï¼š

```
{working_dir}/{namespace}/
â”œâ”€â”€ doc_status.json       # æ–‡æ¡£å¤„ç†çŠ¶æ€ (DocStatusStorage)
â”œâ”€â”€ chunks.json           # æ–‡æœ¬å—å†…å®¹ (KV)
â”œâ”€â”€ llm_cache.json        # LLM å“åº”ç¼“å­˜ (KV)
â”œâ”€â”€ entity_chunks.json    # å®ä½“-å—æ˜ å°„ (KV)
â”œâ”€â”€ relation_chunks.json  # å…³ç³»-å—æ˜ å°„ (KV)
â”œâ”€â”€ full_entities.json    # å®Œæ•´å®ä½“æ•°æ® (KV)
â”œâ”€â”€ full_relations.json   # å®Œæ•´å…³ç³»æ•°æ® (KV)
â”œâ”€â”€ entities_vdb.json     # å®ä½“å‘é‡æ•°æ®
â”œâ”€â”€ relations_vdb.json    # å…³ç³»å‘é‡æ•°æ®
â”œâ”€â”€ chunks_vdb.json       # å—å‘é‡æ•°æ®
â””â”€â”€ graph.graphml         # NetworkX å›¾è°± (GraphML æ ¼å¼)
```

---

## 3. Data Ingestion (æ•°æ®æ‘„å…¥) æµç¨‹è¯¦è§£

### 3.1 é«˜å±‚æµç¨‹å›¾

```mermaid
flowchart TD
    subgraph é˜¶æ®µ1: æ–‡æ¡£é¢„å¤„ç†
        A[ğŸ“„ è¾“å…¥æ–‡æ¡£] --> B[ç”Ÿæˆæ–‡æ¡£ ID<br/>compute_mdhash_id]
        B --> C{æ–‡æ¡£å·²å­˜åœ¨?}
        C -->|æ˜¯| D[æ£€æŸ¥çŠ¶æ€]
        D -->|processed| E[è·³è¿‡]
        D -->|å…¶ä»–| F[é‡æ–°å¤„ç†]
        C -->|å¦| G[åˆ›å»ºæ–°æ–‡æ¡£è®°å½•]
        F --> H[æ›´æ–°çŠ¶æ€ä¸º<br/>processing]
        G --> H
    end

    subgraph é˜¶æ®µ2: æ–‡æœ¬åˆ†å—
        H --> I[chunking_by_token_size]
        I --> J[ç”Ÿæˆå— ID<br/>chunk-{hash}]
        J --> K[å­˜å‚¨åˆ° chunks_kv]
    end

    subgraph é˜¶æ®µ3: å‘é‡åµŒå…¥
        K --> L[ç”Ÿæˆå—åµŒå…¥<br/>embedding_func]
        L --> M[å­˜å‚¨åˆ° chunks_vdb]
    end

    subgraph é˜¶æ®µ4: å®ä½“å…³ç³»æå–
        M --> N[extract_entities<br/>è°ƒç”¨ LLM]
        N --> O[è§£æå®ä½“å’Œå…³ç³»]
        O --> P[ç¼“å­˜ LLM å“åº”]
    end

    subgraph é˜¶æ®µ5: çŸ¥è¯†å›¾è°±æ„å»º
        P --> Q[merge_nodes_and_edges]
        Q --> R[åˆå¹¶å®ä½“æè¿°]
        R --> S[æ›´æ–° graph_storage èŠ‚ç‚¹]
        S --> T[ç”Ÿæˆå®ä½“åµŒå…¥]
        T --> U[æ›´æ–° entities_vdb]
        Q --> V[åˆå¹¶å…³ç³»æè¿°]
        V --> W[æ›´æ–° graph_storage è¾¹]
        W --> X[ç”Ÿæˆå…³ç³»åµŒå…¥]
        X --> Y[æ›´æ–° relations_vdb]
    end

    subgraph é˜¶æ®µ6: å®Œæˆ
        U --> Z[æ›´æ–°çŠ¶æ€ä¸º<br/>processed]
        Y --> Z
        Z --> AA[index_done_callback<br/>æŒä¹…åŒ–]
        AA --> AB[âœ… å®Œæˆ]
    end
```

### 3.2 é˜¶æ®µè¯¦è§£

#### é˜¶æ®µ 1: æ–‡æ¡£é¢„å¤„ç†

**æ ¸å¿ƒä»£ç ä½ç½®**: `lightrag.py` - `apipeline_enqueue_documents()`

```python
# ç”Ÿæˆæ–‡æ¡£å”¯ä¸€ ID
doc_id = compute_mdhash_id(content.strip(), prefix="doc-")

# æ£€æŸ¥æ–‡æ¡£çŠ¶æ€
status = await self.doc_status_storage.get_status_by_id(doc_id)

# çŠ¶æ€ç±»å‹
class DocStatus(Enum):
    PENDING = "pending"       # ç­‰å¾…å¤„ç†
    PROCESSING = "processing" # å¤„ç†ä¸­
    PROCESSED = "processed"   # å·²å®Œæˆ
    FAILED = "failed"         # å¤„ç†å¤±è´¥
```

**å­˜å‚¨å†™å…¥**:

- `doc_status` â†’ æ–‡æ¡£ IDã€çŠ¶æ€ã€æ–‡ä»¶è·¯å¾„ã€æ—¶é—´æˆ³

#### é˜¶æ®µ 2: æ–‡æœ¬åˆ†å—

**æ ¸å¿ƒä»£ç ä½ç½®**: `operate.py` - `chunking_by_token_size()`

```mermaid
flowchart LR
    A[åŸå§‹æ–‡æ¡£<br/>5000 tokens] --> B[åˆ†å—å™¨]
    B --> C{åˆ†å—ç­–ç•¥}
    C -->|æŒ‰ token å¤§å°| D[æ»‘åŠ¨çª—å£åˆ†å—<br/>chunk_token_size=1200<br/>overlap=100]
    C -->|æŒ‰å­—ç¬¦åˆ†å‰²| E[è‡ªå®šä¹‰åˆ†å‰²ç¬¦<br/>å¦‚ç« èŠ‚æ ‡è®°]
    D --> F[Chunk 0<br/>tokens: 1200]
    D --> G[Chunk 1<br/>tokens: 1200]
    D --> H[Chunk N<br/>tokens: ä½™ä¸‹]
    E --> I[æŒ‰åˆ†å‰²ç¬¦å¾—åˆ°çš„å—]
```

**åˆ†å—å‚æ•°**:

- `chunk_token_size`: æ¯å—æœ€å¤§ token æ•° (é»˜è®¤ 1200)
- `chunk_overlap_token_size`: å—é—´é‡å  token æ•° (é»˜è®¤ 100)
- `split_by_character`: è‡ªå®šä¹‰åˆ†å‰²å­—ç¬¦
- `split_by_character_only`: æ˜¯å¦ä»…æŒ‰å­—ç¬¦åˆ†å‰²

**å­˜å‚¨å†™å…¥**:

- `chunks_kv` â†’ chunk_id, content, tokens, full_doc_id, chunk_order_index, file_path

#### é˜¶æ®µ 3: å‘é‡åµŒå…¥

```mermaid
flowchart LR
    A[å—å†…å®¹] --> B[embedding_func<br/>OpenAI/Ollama/...]
    B --> C[å‘é‡<br/>1536ç»´/3072ç»´]
    C --> D[chunks_vdb.upsert]
```

**å­˜å‚¨å†™å…¥**:

- `chunks_vdb` â†’ chunk_id, vector, metadata

#### é˜¶æ®µ 4: å®ä½“å…³ç³»æå–

**æ ¸å¿ƒä»£ç ä½ç½®**: `operate.py` - `extract_entities()`

```mermaid
sequenceDiagram
    participant C as Chunk
    participant P as Prompt Builder
    participant L as LLM
    participant PS as Parser
    participant CA as Cache

    C->>P: å—å†…å®¹ + ä¸Šä¸‹æ–‡
    P->>L: System Prompt + User Prompt
    L-->>PS: æå–ç»“æœ (ç»“æ„åŒ–æ–‡æœ¬)
    PS->>PS: è§£æå®ä½“å’Œå…³ç³»
    L-->>CA: ç¼“å­˜ LLM å“åº”

    Note over L,PS: å¯é€‰: Gleaning (äºŒæ¬¡æå–)
    P->>L: Continue Extraction Prompt
    L-->>PS: è¡¥å……æå–ç»“æœ
```

**LLM æå–æ ¼å¼**:

```
# å®ä½“æ ¼å¼
entity<|#|>å®ä½“åç§°<|#|>å®ä½“ç±»å‹<|#|>å®ä½“æè¿°

# å…³ç³»æ ¼å¼
relation<|#|>æºå®ä½“<|#|>ç›®æ ‡å®ä½“<|#|>å…³é”®è¯<|#|>å…³ç³»æè¿°

# å®Œæˆæ ‡è®°
<|COMPLETE|>
```

**å­˜å‚¨å†™å…¥**:

- `llm_cache` â†’ cache_key, response, create_time, cache_type

#### é˜¶æ®µ 5: çŸ¥è¯†å›¾è°±æ„å»º

**æ ¸å¿ƒä»£ç ä½ç½®**: `operate.py` - `merge_nodes_and_edges()`

```mermaid
flowchart TD
    subgraph å®ä½“å¤„ç†
        A1[æå–çš„å®ä½“åˆ—è¡¨] --> B1[æŒ‰å®ä½“ååˆ†ç»„]
        B1 --> C1{å·²å­˜åœ¨?}
        C1 -->|æ˜¯| D1[åˆå¹¶æè¿°<br/>_handle_entity_relation_summary]
        C1 -->|å¦| E1[åˆ›å»ºæ–°å®ä½“]
        D1 --> F1[ç´¯åŠ  source_ids]
        E1 --> F1
        F1 --> G1[graph_storage.upsert_node]
        G1 --> H1[ç”Ÿæˆå®ä½“åµŒå…¥]
        H1 --> I1[entities_vdb.upsert]
    end

    subgraph å…³ç³»å¤„ç†
        A2[æå–çš„å…³ç³»åˆ—è¡¨] --> B2[æŒ‰ src-tgt åˆ†ç»„]
        B2 --> C2{å·²å­˜åœ¨?}
        C2 -->|æ˜¯| D2[åˆå¹¶æè¿°å’Œå…³é”®è¯]
        C2 -->|å¦| E2[åˆ›å»ºæ–°å…³ç³»]
        D2 --> F2[ç´¯åŠ æƒé‡ weight]
        E2 --> F2
        F2 --> G2[graph_storage.upsert_edge]
        G2 --> H2[ç”Ÿæˆå…³ç³»åµŒå…¥]
        H2 --> I2[relations_vdb.upsert]
    end
```

**æè¿°åˆå¹¶ç­–ç•¥**:

```python
# æ¥è‡ª _handle_entity_relation_summary()

1. å¦‚æœæè¿°æ•° < force_llm_summary_on_merge ä¸” æ€» token < summary_max_tokens:
   â†’ ç›´æ¥æ‹¼æ¥æè¿° (æ— éœ€ LLM)

2. å¦åˆ™ä½¿ç”¨ Map-Reduce ç­–ç•¥:
   â†’ å°†æè¿°åˆ†ç»„ï¼Œæ¯ç»„è°ƒç”¨ LLM æ‘˜è¦
   â†’ é€’å½’å¤„ç†ç›´åˆ°æ»¡è¶³æ¡ä»¶
```

**å­˜å‚¨å†™å…¥**:

- `graph_storage` (èŠ‚ç‚¹) â†’ entity_name, entity_type, description, source_id, file_path
- `graph_storage` (è¾¹) â†’ src_id, tgt_id, weight, description, keywords, source_id
- `entities_vdb` â†’ entity_name, vector, metadata
- `relations_vdb` â†’ relation_key, vector, metadata
- `entity_chunks_kv` â†’ entity_name â†’ [chunk_ids]
- `relation_chunks_kv` â†’ relation_key â†’ [chunk_ids]

---

## 4. Query (æŸ¥è¯¢) æµç¨‹è¯¦è§£

### 4.1 æŸ¥è¯¢æ¨¡å¼

```mermaid
flowchart TB
    subgraph æŸ¥è¯¢æ¨¡å¼
        A[local] --> A1[â†’ å®ä½“+é‚»å±…å—æœç´¢]
        B[global] --> B1[â†’ å®ä½“+å…³ç³»æœç´¢]
        C[hybrid] --> C1[â†’ å®ä½“+å…³ç³»+é‚»å±…å—]
        D[naive] --> D1[â†’ çº¯å‘é‡å—æœç´¢]
        E[mix] --> E1[â†’ æ‰€æœ‰æ¥æºæ··åˆ]
        F[bypass] --> F1[â†’ è·³è¿‡æ£€ç´¢,ç›´æ¥ LLM]
    end

    subgraph æ•°æ®æºä½¿ç”¨
        G[(entities_vdb)]
        H[(relations_vdb)]
        I[(chunks_vdb)]
        J[graph_storage]
    end

    A1 --> G
    A1 --> J
    B1 --> G
    B1 --> H
    B1 --> J
    C1 --> G
    C1 --> H
    C1 --> J
    D1 --> I
    E1 --> G
    E1 --> H
    E1 --> I
    E1 --> J
```

### 4.2 å®Œæ•´æŸ¥è¯¢æµç¨‹

```mermaid
flowchart TD
    subgraph é˜¶æ®µ1: å…³é”®è¯æå–
        A[ç”¨æˆ·æŸ¥è¯¢] --> B[extract_keywords_only<br/>è°ƒç”¨ LLM]
        B --> C[high_level_keywords<br/>å®è§‚æ¦‚å¿µ]
        B --> D[low_level_keywords<br/>å…·ä½“å®ä½“]
    end

    subgraph é˜¶æ®µ2: å¤šæºæœç´¢
        C --> E[æ„å»ºæœç´¢æŸ¥è¯¢]
        D --> E

        subgraph å®ä½“æœç´¢
            E --> F1[entities_vdb.query]
            F1 --> F2[è·å–èŠ‚ç‚¹è¯¦æƒ…]
            F2 --> F3[æŒ‰åº¦æ•°æ’åº]
        end

        subgraph å…³ç³»æœç´¢
            E --> G1[get_node_edges]
            G1 --> G2[è·å–è¾¹è¯¦æƒ…]
            G2 --> G3[æŒ‰æƒé‡æ’åº]
        end

        subgraph å—æœç´¢
            E --> H1[chunks_vdb.query]
            H1 --> H2[è·å–å—å†…å®¹]
        end

        subgraph å®ä½“ç›¸å…³å—
            F3 --> I1[è§£æ source_ids]
            I1 --> I2[chunks_kv.get_by_ids]
        end
    end

    subgraph é˜¶æ®µ3: ä¸Šä¸‹æ–‡æ„å»º
        F3 --> J[åˆå¹¶å»é‡]
        G3 --> J
        H2 --> J
        I2 --> J
        J --> K[Token æˆªæ–­<br/>maxEntityTokens: 6000<br/>maxRelationTokens: 8000]
        K --> L[æ ¼å¼åŒ–ä¸º JSON]
        L --> M[æ„å»ºå¼•ç”¨åˆ—è¡¨]
    end

    subgraph é˜¶æ®µ4: LLM å“åº”ç”Ÿæˆ
        M --> N[æ„å»º System Prompt<br/>RAG æŒ‡ä»¤ + ä¸Šä¸‹æ–‡]
        N --> O[æ·»åŠ å¯¹è¯å†å²]
        O --> P[è°ƒç”¨ LLM]
        P --> Q[è¿”å› QueryResult]
    end
```

### 4.3 æŸ¥è¯¢å‚æ•°è¯¦è§£

```python
@dataclass
class QueryParam:
    mode: Literal["local", "global", "hybrid", "naive", "mix", "bypass"] = "mix"
    only_need_context: bool = False        # ä»…è¿”å›ä¸Šä¸‹æ–‡
    only_need_prompt: bool = False         # ä»…è¿”å› prompt
    stream: bool = False                   # æµå¼è¾“å‡º
    top_k: int = 40                        # æ£€ç´¢æ•°é‡
    chunk_top_k: int = 20                  # å—æ£€ç´¢æ•°é‡
    max_entity_tokens: int = 6000          # å®ä½“æœ€å¤§ token
    max_relation_tokens: int = 8000        # å…³ç³»æœ€å¤§ token
    max_total_tokens: int = 30000          # æ€»æœ€å¤§ token
    hl_keywords: list[str] = []            # é¢„å®šä¹‰é«˜å±‚å…³é”®è¯
    ll_keywords: list[str] = []            # é¢„å®šä¹‰ä½å±‚å…³é”®è¯
    conversation_history: list = []        # å¯¹è¯å†å²
    enable_rerank: bool = True             # å¯ç”¨é‡æ’åº
```

---

## 5. æ ¸å¿ƒæ•°æ®ç»“æ„

### 5.1 æ–‡æ¡£çŠ¶æ€ (DocProcessingStatus)

```python
{
    "id": "doc-a1b2c3...",           # æ–‡æ¡£å”¯ä¸€ ID
    "content_md5": "...",            # å†…å®¹ MD5
    "status": "processed",           # pending/processing/processed/failed
    "created_at": "2024-01-15T...",  # åˆ›å»ºæ—¶é—´
    "updated_at": "2024-01-15T...",  # æ›´æ–°æ—¶é—´
    "file_path": "example.txt",      # æ–‡ä»¶è·¯å¾„
    "error_msg": null,               # é”™è¯¯ä¿¡æ¯
    "chunk_count": 5,                # å—æ•°é‡
}
```

### 5.2 æ–‡æœ¬å— (TextChunkSchema)

```python
{
    "id": "chunk-a1b2c3...",          # å—å”¯ä¸€ ID
    "tokens": 1150,                   # token æ•°é‡
    "content": "å—çš„æ–‡æœ¬å†…å®¹...",      # æ–‡æœ¬å†…å®¹
    "full_doc_id": "doc-...",         # æ‰€å±æ–‡æ¡£ ID
    "chunk_order_index": 0,           # å—é¡ºåºç´¢å¼•
    "file_path": "example.txt",       # æ¥æºæ–‡ä»¶
    "llm_cache_list": ["cache-..."]   # å…³è”çš„ç¼“å­˜ key
}
```

### 5.3 å›¾èŠ‚ç‚¹ (Entity)

```python
{
    "entity_name": "çˆ±å› æ–¯å¦",         # å®ä½“åç§° (ä½œä¸ºèŠ‚ç‚¹ ID)
    "entity_type": "person",          # å®ä½“ç±»å‹
    "description": "å¾·å›½ç‰©ç†å­¦å®¶...",  # æè¿° (å¯åˆå¹¶)
    "source_id": "chunk-a<SEP>chunk-b", # æ¥æºå— IDs
    "file_path": "physics.txt",       # æ¥æºæ–‡ä»¶
}
```

### 5.4 å›¾è¾¹ (Relation)

```python
{
    "src_id": "çˆ±å› æ–¯å¦",             # æºå®ä½“
    "tgt_id": "ç›¸å¯¹è®º",              # ç›®æ ‡å®ä½“
    "weight": 2.5,                   # æƒé‡ (ç´¯åŠ )
    "description": "æå‡ºäº†...",       # å…³ç³»æè¿°
    "keywords": "åˆ›ç«‹,å‘å±•",          # å…³é”®è¯
    "source_id": "chunk-a<SEP>chunk-b", # æ¥æºå— IDs
    "file_path": "physics.txt",       # æ¥æºæ–‡ä»¶
}
```

### 5.5 å‘é‡å­˜å‚¨æ•°æ®

```python
# entities_vdb / relations_vdb / chunks_vdb
{
    "id": "entity_name æˆ– chunk_id",
    "vector": [0.1, 0.2, ...],        # åµŒå…¥å‘é‡
    "metadata": {
        "entity_name": "...",
        "description": "...",
        # å…¶ä»–å…ƒæ•°æ®
    }
}
```

---

## 6. LLM Prompt æ¨¡æ¿

### 6.1 å®ä½“æå– Prompt

```
---Role---
You are a Knowledge Graph Specialist responsible for extracting
entities and relationships from the input text.

---Instructions---
1. Entity Extraction & Output:
   - Identification: Identify clearly defined and meaningful entities
   - Entity Details: entity_name, entity_type, entity_description
   - Format: entity<|#|>name<|#|>type<|#|>description

2. Relationship Extraction & Output:
   - Identification: Identify direct, clearly stated relationships
   - Relationship Details: source, target, keywords, description
   - Format: relation<|#|>source<|#|>target<|#|>keywords<|#|>description

3. Completion: Output <|COMPLETE|> as the final line
```

### 6.2 å…³é”®è¯æå– Prompt

```
Given the query, extract:
- High-level keywords: Broad concepts for understanding the query scope
- Low-level keywords: Specific entities, names, or concrete terms

Output format:
{
    "high_level_keywords": [...],
    "low_level_keywords": [...]
}
```

### 6.3 RAG å“åº”ç”Ÿæˆ Prompt

```
---Role---
You are a helpful assistant responding to user queries using
the provided Knowledge Graph and document chunks.

---Data Sources---
1. Knowledge Graph Entities: [...]
2. Knowledge Graph Relations: [...]
3. Document Chunks: [...]

---Query---
{user_query}

---Instructions---
- Use the provided data to answer accurately
- Cite sources using reference IDs
- If information is insufficient, acknowledge limitations
```

---

## 7. å…³é”®é…ç½®å¸¸é‡

```python
# æ¥è‡ª constants.py

# åˆ†å—é…ç½®
DEFAULT_CHUNK_TOKEN_SIZE = 1200
DEFAULT_CHUNK_OVERLAP = 100

# æå–é…ç½®
DEFAULT_MAX_GLEANING = 1              # äºŒæ¬¡æå–æ¬¡æ•°
DEFAULT_ENTITY_NAME_MAX_LENGTH = 256  # å®ä½“åæœ€å¤§é•¿åº¦
DEFAULT_ENTITY_TYPES = [
    "Person", "Creature", "Organization", "Location",
    "Event", "Concept", "Method", "Content", "Data",
    "Artifact", "NaturalObject"
]

# æè¿°æ‘˜è¦é…ç½®
DEFAULT_FORCE_LLM_SUMMARY_ON_MERGE = 8   # è§¦å‘ LLM æ‘˜è¦çš„æè¿°æ•°
DEFAULT_SUMMARY_MAX_TOKENS = 1200        # è§¦å‘æ‘˜è¦çš„æœ€å¤§ token
DEFAULT_SUMMARY_LENGTH_RECOMMENDED = 600 # æ¨èæ‘˜è¦é•¿åº¦
DEFAULT_SUMMARY_CONTEXT_SIZE = 12000     # æ‘˜è¦ä¸Šä¸‹æ–‡æœ€å¤§ token

# æŸ¥è¯¢é…ç½®
DEFAULT_TOP_K = 40
DEFAULT_CHUNK_TOP_K = 20
DEFAULT_MAX_ENTITY_TOKENS = 6000
DEFAULT_MAX_RELATION_TOKENS = 8000
DEFAULT_MAX_TOTAL_TOKENS = 30000
DEFAULT_COSINE_THRESHOLD = 0.2

# Source ID ç®¡ç†
DEFAULT_MAX_SOURCE_IDS_PER_ENTITY = 300
DEFAULT_MAX_SOURCE_IDS_PER_RELATION = 300
GRAPH_FIELD_SEP = "<SEP>"                # source_id åˆ†éš”ç¬¦

# å¼‚æ­¥é…ç½®
DEFAULT_MAX_ASYNC = 4
DEFAULT_MAX_PARALLEL_INSERT = 2
DEFAULT_EMBEDDING_FUNC_MAX_ASYNC = 8
DEFAULT_EMBEDDING_BATCH_NUM = 10

# è¶…æ—¶é…ç½®
DEFAULT_LLM_TIMEOUT = 180
DEFAULT_EMBEDDING_TIMEOUT = 30
```

---

## 8. å­˜å‚¨æ¥å£æŠ½è±¡

### 8.1 BaseKVStorage

```python
class BaseKVStorage(ABC):
    """é”®å€¼å­˜å‚¨æŠ½è±¡åŸºç±»"""

    @abstractmethod
    async def get_by_id(self, id: str) -> dict | None:
        """æ ¹æ® ID è·å–å€¼"""

    @abstractmethod
    async def get_by_ids(self, ids: list[str]) -> list[dict]:
        """æ‰¹é‡è·å–"""

    @abstractmethod
    async def filter_keys(self, keys: set[str]) -> set[str]:
        """è¿‡æ»¤ä¸å­˜åœ¨çš„ keys"""

    @abstractmethod
    async def upsert(self, data: dict[str, dict]) -> None:
        """æ’å…¥æˆ–æ›´æ–°"""

    @abstractmethod
    async def delete(self, ids: list[str]) -> None:
        """åˆ é™¤"""
```

### 8.2 BaseVectorStorage

```python
class BaseVectorStorage(ABC):
    """å‘é‡å­˜å‚¨æŠ½è±¡åŸºç±»"""

    embedding_func: EmbeddingFunc
    cosine_better_than_threshold: float = 0.2

    @abstractmethod
    async def query(
        self, query: str, top_k: int,
        query_embedding: list[float] = None
    ) -> list[dict]:
        """è¯­ä¹‰æŸ¥è¯¢"""

    @abstractmethod
    async def upsert(self, data: dict[str, dict]) -> None:
        """æ’å…¥æˆ–æ›´æ–°å‘é‡"""

    @abstractmethod
    async def delete(self, ids: list[str]) -> None:
        """åˆ é™¤å‘é‡"""
```

### 8.3 BaseGraphStorage

```python
class BaseGraphStorage(ABC):
    """å›¾å­˜å‚¨æŠ½è±¡åŸºç±»"""

    @abstractmethod
    async def has_node(self, node_id: str) -> bool:
        """æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦å­˜åœ¨"""

    @abstractmethod
    async def has_edge(self, src: str, tgt: str) -> bool:
        """æ£€æŸ¥è¾¹æ˜¯å¦å­˜åœ¨"""

    @abstractmethod
    async def get_node(self, node_id: str) -> dict | None:
        """è·å–èŠ‚ç‚¹"""

    @abstractmethod
    async def get_edge(self, src: str, tgt: str) -> dict | None:
        """è·å–è¾¹"""

    @abstractmethod
    async def get_node_edges(self, node_id: str) -> list[tuple]:
        """è·å–èŠ‚ç‚¹çš„æ‰€æœ‰è¾¹"""

    @abstractmethod
    async def upsert_node(self, node_id: str, node_data: dict) -> None:
        """æ’å…¥æˆ–æ›´æ–°èŠ‚ç‚¹"""

    @abstractmethod
    async def upsert_edge(
        self, src: str, tgt: str, edge_data: dict
    ) -> None:
        """æ’å…¥æˆ–æ›´æ–°è¾¹"""

    @abstractmethod
    async def node_degree(self, node_id: str) -> int:
        """è·å–èŠ‚ç‚¹åº¦æ•°"""
```

---

## 9. å¹¶å‘å’Œé”æœºåˆ¶

### 9.1 å…±äº«å­˜å‚¨é”

```mermaid
flowchart TD
    subgraph é”ç±»å‹
        A[UnifiedLock<br/>ç»Ÿä¸€é”æ¥å£]
        B[KeyedUnifiedLock<br/>é”®æ§é”ç®¡ç†å™¨]
    end

    subgraph é”ç²’åº¦
        C[å‘½åç©ºé—´é”<br/>namespace_lock]
        D[æ•°æ®åˆå§‹åŒ–é”<br/>data_init_lock]
        E[å­˜å‚¨é”®æ§é”<br/>storage_keyed_lock]
    end

    subgraph ä½¿ç”¨åœºæ™¯
        F[å¤šè¿›ç¨‹å®‰å…¨<br/>Gunicorn workers]
        G[å¼‚æ­¥ä»»åŠ¡å®‰å…¨<br/>asyncio tasks]
        H[å®ä½“/å…³ç³»æ›´æ–°<br/>å¹¶å‘å†™å…¥ä¿æŠ¤]
    end

    A --> C
    A --> D
    B --> E
    C --> F
    D --> F
    E --> G
    E --> H
```

### 9.2 å¹¶å‘æ§åˆ¶

```python
# æ¥è‡ª operate.py

# å®ä½“æå–å¹¶å‘
chunk_max_async = global_config.get("llm_model_max_async", 4)
semaphore = asyncio.Semaphore(chunk_max_async)

# å›¾æ“ä½œå¹¶å‘
graph_max_async = global_config.get("llm_model_max_async", 4) * 2

# å®ä½“/å…³ç³»æ›´æ–°æ—¶ä½¿ç”¨é”®æ§é”
async with get_storage_keyed_lock(
    [entity_name],
    namespace="GraphDB",
    enable_logging=False
):
    await _merge_nodes_then_upsert(...)
```

---

## 10. å®Œæ•´ä½¿ç”¨ç¤ºä¾‹

```python
from lightrag import LightRAG

async def main():
    # 1. åˆå§‹åŒ–
    rag = LightRAG(
        working_dir="./my_rag_storage",
        namespace="demo",
        kv_storage="JsonKVStorage",
        vector_storage="NanoVectorDBStorage",
        graph_storage="NetworkXStorage",
        llm_model_func=my_llm_func,
        embedding_func=my_embedding_func,
        entity_types=["Person", "Organization", "Event"],
        language="Chinese",
    )
    await rag.initialize_storages()

    # 2. æ’å…¥æ–‡æ¡£
    await rag.ainsert("""
        çˆ±å› æ–¯å¦äº1905å¹´å‘è¡¨äº†ç‹­ä¹‰ç›¸å¯¹è®ºã€‚
        è¿™ä¸€ç†è®ºå½»åº•æ”¹å˜äº†ç‰©ç†å­¦çš„åŸºç¡€ã€‚
        1921å¹´ï¼Œä»–å› å…‰ç”µæ•ˆåº”è·å¾—è¯ºè´å°”ç‰©ç†å­¦å¥–ã€‚
    """, file_paths="physics.txt")

    # 3. æŸ¥è¯¢
    result = await rag.aquery(
        "çˆ±å› æ–¯å¦è·å¾—äº†ä»€ä¹ˆå¥–é¡¹ï¼Ÿ",
        param=QueryParam(mode="hybrid", top_k=10)
    )
    print(result)

    # 4. è·å–çŸ¥è¯†å›¾è°±
    kg = await rag.get_knowledge_graph("çˆ±å› æ–¯å¦", max_depth=2)
    print(f"èŠ‚ç‚¹: {len(kg.nodes)}, è¾¹: {len(kg.edges)}")

    # 5. æ¸…ç†
    await rag.finalize_storages()

asyncio.run(main())
```

---

## 11. æ€»ç»“

LightRAG æ˜¯ä¸€ä¸ªåŠŸèƒ½å®Œå¤‡çš„ RAG æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒç‰¹ç‚¹åŒ…æ‹¬ï¼š

1. **æ¨¡å—åŒ–å­˜å‚¨**: æ”¯æŒå¤šç§å­˜å‚¨åç«¯ (JSON/Redis/PostgreSQL/MongoDB/Neo4j ç­‰)
2. **çŸ¥è¯†å›¾è°±å¢å¼º**: ä¸ä»…ä½¿ç”¨å‘é‡æ£€ç´¢ï¼Œè¿˜æ„å»ºçŸ¥è¯†å›¾è°±æå‡æ£€ç´¢è´¨é‡
3. **æ™ºèƒ½æè¿°åˆå¹¶**: ä½¿ç”¨ Map-Reduce ç­–ç•¥å¤„ç†é‡å¤å®ä½“/å…³ç³»çš„æè¿°
4. **çµæ´»æŸ¥è¯¢æ¨¡å¼**: æ”¯æŒ local/global/hybrid/naive/mix å¤šç§æŸ¥è¯¢ç­–ç•¥
5. **ä¼ä¸šçº§ç‰¹æ€§**: æ”¯æŒå¤šè¿›ç¨‹ã€å¼‚æ­¥å¹¶å‘ã€ç¼“å­˜ã€é‡æ’åºç­‰
6. **å¯æ‰©å±•æ¶æ„**: æ¸…æ™°çš„æŠ½è±¡æ¥å£ä¾¿äºæ·»åŠ æ–°çš„å­˜å‚¨å®ç°
